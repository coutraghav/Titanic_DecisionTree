# -*- coding: utf-8 -*-
"""Titanic_Decision_Treeipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xzX8cW0OCnbe1gy0-uLymLdD8VT3Czuq
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

data=pd.read_csv("train_titanic1.csv")
data.head()

columns_to_drop=["PassengerId","Name","Ticket","Cabin","Embarked"]
data_clean=data.drop(columns_to_drop,axis=1)
data_clean.head()

data_clean.info()

le=LabelEncoder()
data_clean["Sex"]=le.fit_transform(data_clean["Sex"])

data_clean.head()

data_clean=data_clean.fillna(data_clean["Age"].mean())
data_clean.info()

input_cols=['Pclass','Sex','Age','SibSp',"Parch",'Fare']
output_cols=['Survived']

X=data_clean[input_cols]
Y=data_clean[output_cols]
print(X,Y)

def entropy(col):
  N=col.shape[0]
  counts=np.unique(col,return_counts=True)

  ent=0.0

  for i in counts[1]:
    p=i/N
    ent+= (-1.0*p*np.log2(p))

  return ent

def divide_data(x_data,fkey,fval):

  x_right=pd.DataFrame([],columns=x_data.columns)
  x_left=pd.DataFrame([],columns=x_data.columns)

  for i in range(x_data.shape[0]):
    
    val=x_data[fkey].loc[i]

    if val>fval:
      x_right=x_right.append(x_data.loc[i])

    else:
      x_left=x_left.append(x_data.loc[i])
    

  return x_left,x_right

def information_gain(x_data,fkey,fval):

  left,right=divide_data(x_data,fkey,fval)

  if left.shape[0]==0 or right.shape[0]==0:
    return -1000000
  
  l=float(left.shape[0])/x_data.shape[0]
  r=float(right.shape[0])/x_data.shape[0]
  
  i_gain=entropy(x_data.Survived)-(l*entropy(left.Survived)+r*entropy(right.Survived))
  
  return i_gain

for fx in X.columns:
  print(fx)
  print(information_gain(data_clean,fx,data_clean[fx].mean()))

a=data_clean.columns[:-1]
a

class DecisionTree:


  def __init__(self,depth=0,max_depth=5):

    self.left=None
    self.right=None
    self.fkey=None
    self.fval=None
    self.max_depth=max_depth
    self.depth=depth
    self.target=None
  

  def train(self,X_train):

    features=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']
    info_gains=list()
    
    for ix in features:
      i_gain=information_gain(X_train,ix,X_train[ix].mean())
      info_gains.append(i_gain)

    self.fkey=features[np.argmax(info_gains)]
    self.fval=X_train[self.fkey].mean()
    print("Making Tree Feature : ",self.fkey)

    data_left,data_right=divide_data(X_train,self.fkey,self.fval)

    data_left=data_left.reset_index(drop=True)
    data_right=data_right.reset_index(drop=True)

    if data_left.shape[0]==0 or data_right.shape[0]==0:

      if X_train.Survived.mean()>=0.5:
        self.target="Survive"
      else:
        self.target="Dead"
      return


    if(self.depth>=self.max_depth):
      if X_train.Survived.mean()>=0.5:
        self.target="Survive"
      else:
        self.target="Dead"
      return

    self.left=DecisionTree(depth=self.depth+1,max_depth=self.max_depth)
    self.left.train(data_left)  

    self.right=DecisionTree(depth=self.depth+1,max_depth=self.max_depth)
    self.right.train(data_right)

  

    if X_train.Survived.mean()>=0.5:
      self.target="Survive"
    else:
      self.target="Dead"
    return

  def predict(self,test):

    if test[self.fkey]>self.fval:

      if self.right is None:
        return self.target
      return self.right.predict(test)

    else:
      if self.left is None:
        return self.target
      return self.left.predict(test)

d=DecisionTree()
d.train(data_clean)

test_data=pd.read_csv("test_titanic.csv")
test_data.info()

columns_to_drop=["PassengerId","Name","Ticket","Cabin","Embarked"]
test_data_clean=test_data.drop(columns_to_drop,axis=1)
test_data_clean.head()